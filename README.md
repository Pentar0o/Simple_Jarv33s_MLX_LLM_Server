# MLX OpenAI Compatible API Server

Un serveur API compatible avec le format OpenAI qui ex√©cute des mod√®les de langage localement sur les Mac avec Apple Silicon en utilisant MLX.

## üåü Fonctionnalit√©s

- üöÄ **Compatible OpenAI** : Impl√©mente l'API `/v1/chat/completions` standard d'OpenAI
- üçé **Optimis√© pour Apple Silicon** : Utilise MLX pour une inf√©rence rapide sur les puces Apple
- üî• **Local** : Ex√©cution 100% locale pour la confidentialit√© et le fonctionnement hors ligne
- üß† **Flexible** : Supporte diff√©rents mod√®les compatibles MLX (Llama, Mistral, etc.)
- üå°Ô∏è **Param√©trable** : Contr√¥le de la temp√©rature, du nombre de tokens, etc.

## üìã Pr√©requis

- Mac avec puce Apple Silicon (M1, M2, M3, etc.)
- Python 3.10+
- pip3

## ‚öôÔ∏è Installation

1. Clonez le d√©p√¥t :
```bash
git clone https://github.com/VOTRE_NOM_UTILISATEUR/mlx-openai-server.git
cd mlx-openai-server
```

2. Installez les d√©pendances :
```bash
pip3 install flask flask_session mlx-lm huggingface_hub --break-system-packages
```

## üöÄ D√©marrage rapide

Lancez le serveur avec le mod√®le par d√©faut :

```bash
python3 server.py
```

Ou sp√©cifiez un mod√®le personnalis√© et d'autres options :

```bash
python3 server.py --model mlx-community/Llama-3.2-8B-Instruct-4bit --temperature 0.7 --port 1982
```

## üîß Options de configuration

Le serveur accepte les options suivantes :

| Option | Description | Par d√©faut |
|--------|-------------|------------|
| `--model` | Nom du mod√®le MLX √† utiliser | `mlx-community/Llama-3.2-3B-Instruct-4bit` |
| `--temperature` | Temp√©rature pour la g√©n√©ration (0.0 = d√©terministe) | `0.0` |
| `--host` | Adresse IP du serveur | `127.0.0.1` |
| `--port` | Port d'√©coute | `1982` |

## üìù Exemples d'utilisation

### cURL

```bash
curl http://localhost:1982/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",
    "messages": [{"role": "user", "content": "Explique-moi la relativit√© restreinte en termes simples"}],
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

### Python

```python
import requests

response = requests.post(
    "http://localhost:1982/v1/chat/completions",
    json={
        "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",
        "messages": [
            {"role": "system", "content": "Tu es un assistant utile et pr√©cis."},
            {"role": "user", "content": "Quels sont les meilleurs langages de programmation pour l'IA en 2025?"}
        ],
        "temperature": 0.3
    }
)

print(response.json()["choices"][0]["message"]["content"])
```

### Int√©gration avec des clients compatibles OpenAI

Le serveur peut √™tre utilis√© avec n'importe quelle biblioth√®que cliente compatible OpenAI en rempla√ßant simplement l'URL de base.

```python
from openai import OpenAI

client = OpenAI(
    api_key="FAUX-CL√â-API",  # N'importe quelle cha√Æne fonctionne
    base_url="http://localhost:1982/v1/"
)

response = client.chat.completions.create(
    model="mlx-community/Llama-3.2-3B-Instruct-4bit",
    messages=[
        {"role": "user", "content": "√âcris un court po√®me sur la programmation"}
    ]
)

print(response.choices[0].message.content)
```

## üìä Mod√®les test√©s

Le serveur a √©t√© test√© avec les mod√®les suivants :

- `mlx-community/Llama-3.2-3B-Instruct-4bit`
- `mlx-community/Mistral-Small-24B-Instruct-2501-bf16`
- `mlx-community/Mistral-Nemo-Instruct-2407-bf16`

## ‚ö†Ô∏è Limitations

- Le support des fonctions/outils n'est pas encore impl√©ment√©
- Certains mod√®les MLX peuvent avoir des formats de messages diff√©rents
- Les performances d√©pendent de la puce Apple Silicon et de la taille du mod√®le

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou une pull request.

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier LICENSE pour plus de d√©tails.
